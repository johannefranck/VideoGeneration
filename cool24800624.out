{'save_dir': 'results/teapot', 'num_samples': 1, 'input_dir': './data/teapot', 'log_freq': 0, 'sd_model_config': 'configs/stable-diffusion/v1-inference.yaml', 'ckpt': './chkpts/sd-v1-4.ckpt', 'ddim_steps': 500, 'ddim_eta': 0.0, 'scale': 7.5, 'prompt': 'Floating teapot', 'target_flow_name': None, 'edit_mask_path': '', 'guidance_weight': 300.0, 'num_recursive_steps': 1, 'color_weight': 100.0, 'flow_weight': 3.0, 'oracle_flow': False, 'no_occlusion_masking': False, 'no_init_startzt': False, 'use_cached_latents': False, 'guidance_schedule_path': 'data/guidance_schedule.npy', 'clip_grad': 200.0}
Loading model from ./chkpts/sd-v1-4.ckpt
Global Step: 470000
LatentDiffusion: Running in eps-prediction mode
DiffusionWrapper has 859.52 M params.
making attention of type 'vanilla' with 512 in_channels
Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
making attention of type 'vanilla' with 512 in_channels

Processing flow: down150.mask.pth

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 24800624: <vidgen1> in cluster <dcc> Exited

Job <vidgen1> was submitted from host <gbarlogin1> by user <s201390> in cluster <dcc> at Fri Apr 25 13:50:36 2025
Job was executed on host(s) <n-62-20-3>, in queue <gpuv100>, as user <s201390> in cluster <dcc> at Fri Apr 25 13:53:12 2025
</zhome/ee/d/152333> was used as the home directory.
</work3/s201390/VideoGeneration> was used as the working directory.
Started at Fri Apr 25 13:53:12 2025
Terminated at Fri Apr 25 13:53:49 2025
Results reported at Fri Apr 25 13:53:49 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -J vidgen1           
#BSUB -q gpuv100           
#BSUB -W 01:30                     # Wall time: 30 minutes
#BSUB -R "rusage[mem=20GB]"    
#BSUB -o cool%J.out            
#BSUB -e cool%J.err            

#BSUB -n 1
#BSUB -R "span[hosts=1]"          

# Load CUDA module (update if needed)
module load cuda/11.7

# Activate the local virtual environment
#source /work3/s201390/ADLCV/VideoGeneration/
source motion_guidance_env/bin/activate

# Optional: set PYTHONPATH if custom modules are needed
#export PYTHONPATH=/work3/s204104/ADLCV/VideoGeneration:$PYTHONPATH

## For memory
export HOME=$PWD

# Ensure cache & config folders exist (just in case)
mkdir -p "$HOME/.cache" "$HOME/.config" "$HOME/.huggingface"


# Run your script
#python ./generate.py \
#    --prompt "an apple on a wooden table" \
#    --input_dir ./data/apple \
#    --edit_mask_path right.mask.pth \
#    --target_flow_name right.pth \
#    --use_cached_latents \
#    --save_dir results/apple.right.rec_step_1 \
#    --log_freq 5 \
#    --num_recursive_steps 1


#export PYTHONPATH=$PYTHONPATH:/work3/s201390/VideoGeneration/motion_guidance_env/src/taming-transformers
export PYTHONPATH=/VideoGeneration/motion_guidance_env/src/taming-transformers

export PYTHONPATH=/work3/s201390/VideoGeneration/motion_guidance_env/src/taming-transformers
# Run your script
python ./generate.py \

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   24.19 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     20480.00 MB
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              -
    Max Threads :                                -
    Run time :                                   0 sec.
    Turnaround time :                            193 sec.

The output (if any) is above this job summary.



PS:

Read file <cool24800624.err> for stderr output of this job.

